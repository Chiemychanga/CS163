Larry Chiem 
CS163 Program 3 Efficiency Review Write-up
Using a hash table data structure for this program is good for this application of study guides, since we only have a couple of keywords that we studied for. It’s not like we needed to study over 1000 keywords, but the data structures we are studying there’s only like less than maybe 20? So the amount of collisions with a table size of 5 as a prime number is not too heavy. The problem comes when we start considering that the table size is determined before run-time, and as we add more and more data, what happens when the collisions get out of hand? The amount of traversals increases to for removing and searching by O(N). Seeing in data structures we have only needed to learn about arrays, linked lists, binary search trees, 2-3-4 trees, black trees, and maybe some more I’m not mentioning, the list is not too large. But considering a bigger study guide, maybe this data structure is not the best for this application.  
A hash table works fine for organizing by keywords and direct access. The only problem with organizing another hash table by the chapter number is that the number of collisions for keywords would have a bunch of collisions. Because we needed to also make a hash table sorted by the chapters, I believe a binary search tree for just the chapters would be fine. All of the chapters would be organized from lowest on the left of the tree, and the highest chapters on the right. 
What was efficient about my design and use of the hash tables structure was at best insertion,search, and deletion would take O(1) constant time, and worse would take O(n) time. I also used a recursive remove function which definitely helped make my code more efficient for such a small data size. If I had done it iteratively, I was getting seg faults after removing the last one and that was just partly my fault for not considering other cases. But when I did it recursively, it handled the NULLs and prevented my pointers pointing to garbage. 
What was not efficient about my design and use of hash tables was that I have to know the approximate size of input data before initializing hash table size. It can be a time-consuming operation. Also, the nodes stored in our hash table were unsorted, so searching and deletion would require us to traverse and compare throughout each traversal for matches. When I was accessing the hash table that was organized by the chapters, there will always be more collisions for chapters versus the hash table organized by the keyword. Chapter N may have hundreds of keywords, so I’d have to traverse through each list to find the keyword that I’m looking for. 
        If I would have done it differently, I would have just created a Binary search tree for inserting by chapters and using a hash table for the keywords. Also, if I wanted to achieve the best performance for chaining in my hash table, each chain should be about the same length (total number of items / total size of table). My hash table had a set array size and that’s not the best for a program like this, where I may learn new keywords and more collisions would occur when I need it to be flexible. A BST would have only allocated as much space as it was needed, which is why it’s more memory-efficient for the chapters. Also, it would have put my data in sorted order and displayed in sorted order. With a BST, I can also do range searches efficiently, meanwhile in a hash table you can’t.